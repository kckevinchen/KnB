{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from net.util import *\n",
        "import os\n",
        "import hypertools as hyp\n",
        "import pickle\n",
        "import json\n",
        "from sklearn.preprocessing import Normalizer\n",
        "import numpy as np\n",
        "import faiss\n",
        "from  tqdm import tqdm\n",
        "\n",
        "def aa(*args, **kwargs):\n",
        "    group.add_argument(*args, **kwargs)\n",
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "group = parser.add_argument_group('dataset options')\n",
        "aa(\"--database\", default=\"deep1b\") # can be \"bigann\", \"deep1b\" or \"*.fvecs\"\n",
        "aa(\"--size_base\", type=int, default=int(1e6),\n",
        "    help=\"size of evaluation dataset\")\n",
        "aa(\"--num_learn\", type=int, default=int(5e5),\n",
        "    help=\"nb of learning vectors\")\n",
        "\n",
        "group = parser.add_argument_group('Model hyperparameters')\n",
        "aa(\"--dint\", type=int, default=1024,\n",
        "    help=\"size of hidden states\")\n",
        "aa(\"--dout\", type=int, default=16,\n",
        "    help=\"output dimension\")\n",
        "aa(\"--lambda_uniform\", type=float, default=0.05,\n",
        "    help=\"weight of the uniformity loss\")\n",
        "aa(\"--lambda_seperation\", type=float, default=0.05,\n",
        "    help=\"weight of the seperation loss\")\n",
        "aa(\"--lambda_lid\", type=float, default=0.01,\n",
        "    help=\"weight of the seperation loss\")\n",
        "aa(\"--lambda_triplet\", type=float, default=1,\n",
        "    help=\"weight of the seperation loss\")\n",
        "\n",
        "    \n",
        "group = parser.add_argument_group('Training hyperparameters')\n",
        "aa(\"--batch_size\", type=int, default=64)\n",
        "aa(\"--epochs\", type=int, default=160)\n",
        "aa(\"--momentum\", type=float, default=0.9)\n",
        "aa(\"--rank_positive\", type=int, default=10,\n",
        "    help=\"this number of vectors are considered positives\")\n",
        "aa(\"--rank_negative\", type=int, default=50,\n",
        "    help=\"these are considered negatives\")\n",
        "\n",
        "group = parser.add_argument_group('Computation params')\n",
        "aa(\"--seed\", type=int, default=1234)\n",
        "aa(\"--checkpoint_dir\", type=str, default=\"\",\n",
        "    help=\"checkpoint directory\")\n",
        "aa(\"--init_name\", type=str, default=\"\",\n",
        "    help=\"checkpoint to load from\")\n",
        "aa(\"--save_best_criterion\", type=str, default=\"\",\n",
        "    help=\"for example r2=4,rank=10\")\n",
        "aa(\"--quantizer_train\", type=str, default=\"\")\n",
        "aa(\"--lr_schedule\", type=str, default=\"0.1,0.05,0.01\")\n",
        "aa(\"--device\", choices=[\"cuda\", \"cpu\", \"auto\"], default=\"auto\")\n",
        "aa(\"--val_freq\", type=int, default=10,\n",
        "    help=\"frequency of validation calls\")\n",
        "aa(\"--validation_quantizers\", type=str, default=\"\",\n",
        "    help=\"r2 values to try in validation\")\n",
        "aa(\"--update_freq\",type=int,default=10,help=\"update frequence\")\n",
        "\n",
        "aa(\"--margin_k\",type=int,default=0,help=\"update frequence\")\n",
        "\n",
        "aa(\"--mode\", type=str,default=\"None\")\n",
        "aa('--alpha', '-a', type=float, default=0.12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_data = np.load('./data/wiki_data_film/train_data.npy')\n",
        "train_labels = np.load('./data/wiki_data_film/train_labels.npy')\n",
        "test_data = np.load('./data/wiki_data_film/test_data.npy')\n",
        "test_labels = np.load('./data/wiki_data_film/test_labels.npy')\n",
        "\n",
        "all_gt = np.load('./data/wiki_data_film/gt.npy')\n",
        "\n",
        "print(\"finish loading\")\n",
        "np.random.seed(1234)\n",
        "tsize = 100000\n",
        "tidx = np.random.choice(train_data.shape[0], tsize, replace=False)\n",
        "\n",
        "\n",
        "xt_labels = train_labels[tidx]\n",
        "xb_labels = train_labels\n",
        "xb = train_data\n",
        "xt = train_data[tidx]\n",
        "\n",
        "imp_class = 1\n",
        "\n",
        "imp_idx = xt_labels == imp_class \n",
        "test_imp_idx = test_labels == imp_class \n",
        "xb_imp = xb_labels == imp_class \n",
        "  \n",
        "\n",
        "imp_test = np.nonzero(test_imp_idx)[0]\n",
        "imp_xb = np.nonzero(xb_imp)[0]\n",
        "\n",
        "dpr = []\n",
        "xq_all = {}\n",
        "gt_all = {}\n",
        "all_skewness = [0.7]\n",
        "l = [64]\n",
        "for skewness in all_skewness:\n",
        "  print(\"generating data for skwness {}\".format(skewness))\n",
        "  other_size = int(imp_test.shape[0]/skewness) - imp_test.shape[0]\n",
        "  other_test = np.setdiff1d(np.arange(test_data.shape[0]),imp_test)\n",
        "  other_idx = np.random.choice(other_test.shape[0], other_size , replace=False)\n",
        "  other_test = other_test[other_idx]\n",
        "  test_idx = np.concatenate([other_test,imp_test])\n",
        "\n",
        "  xq = test_data[test_idx]\n",
        "  print(xq.shape)\n",
        "  gt = all_gt[test_idx,:]\n",
        "  xq_all[skewness] = xq\n",
        "  gt_all[skewness] = gt\n",
        "\n",
        "\n",
        "opq_res = generate_opq(l,xt,xq_all,xb,imp_idx,gt_all,all_skewness=all_skewness)\n",
        "opq_adp = generate_opq_adaptive(l,xt,xq_all,xb,imp_idx,gt_all,all_skewness=all_skewness)\n",
        "\n",
        "hnsw = generate_hnsw([15],xt,xq_all,xb,imp_idx,gt_all,all_skewness=all_skewness)\n",
        "\n",
        "\n",
        "c_dir = \"./wiki_film/model\"\n",
        "if(not os.path.exists(c_dir)):\n",
        "  os.makedirs(c_dir)\n",
        "args = parser.parse_args(\"--lr_schedule 0.1,0.1,0.05,0.01 --num_learn 100000 --database bigann --update_freq 20 --val_freq 10 --lambda_triplet 10 --margin_k 10 --lambda_uniform 0.1 --lambda_lid 0.05 --lambda_seperation 0.01   --dint 1024 --dout 64 --size_base 5000000 --batch_size 512 --epochs 60 --save_best_criterion opqAdaptive_64,rank=100\".split())\n",
        "log_res = generate_result(l,xt,xq_all,xb,imp_idx,xb_imp,gt_all,args=args,dir=c_dir,all_skewness=all_skewness)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_data = np.load('./data/wiki_data_soccer/train_data.npy')\n",
        "train_labels = np.load('./data/wiki_data_soccer/train_labels.npy')\n",
        "test_data = np.load('./data/wiki_data_soccer/test_data.npy')\n",
        "test_labels = np.load('./data/wiki_data_soccer/test_labels.npy')\n",
        "\n",
        "all_gt = np.load('./data/wiki_data_soccer/gt.npy')\n",
        "\n",
        "print(\"finish loading\")\n",
        "np.random.seed(1234)\n",
        "tsize = 100000\n",
        "tidx = np.random.choice(train_data.shape[0], tsize, replace=False)\n",
        "\n",
        "\n",
        "xt_labels = train_labels[tidx]\n",
        "xb_labels = train_labels\n",
        "xb = train_data\n",
        "xt = train_data[tidx]\n",
        "\n",
        "imp_class = 1\n",
        "\n",
        "imp_idx = xt_labels == imp_class \n",
        "test_imp_idx = test_labels == imp_class \n",
        "xb_imp = xb_labels == imp_class \n",
        "  \n",
        "\n",
        "imp_test = np.nonzero(test_imp_idx)[0]\n",
        "imp_xb = np.nonzero(xb_imp)[0]\n",
        "\n",
        "dpr = []\n",
        "xq_all = {}\n",
        "gt_all = {}\n",
        "all_skewness = [0.7]\n",
        "l = [64]\n",
        "for skewness in all_skewness:\n",
        "  print(\"generating data for skwness {}\".format(skewness))\n",
        "  other_size = int(imp_test.shape[0]/skewness) - imp_test.shape[0]\n",
        "  other_test = np.setdiff1d(np.arange(test_data.shape[0]),imp_test)\n",
        "  other_idx = np.random.choice(other_test.shape[0], other_size , replace=False)\n",
        "  other_test = other_test[other_idx]\n",
        "  test_idx = np.concatenate([other_test,imp_test])\n",
        "\n",
        "  xq = test_data[test_idx]\n",
        "  print(xq.shape)\n",
        "  gt = all_gt[test_idx,:]\n",
        "  xq_all[skewness] = xq\n",
        "  gt_all[skewness] = gt\n",
        "\n",
        "\n",
        "opq_res = generate_opq(l,xt,xq_all,xb,imp_idx,gt_all,all_skewness=all_skewness)\n",
        "opq_adp = generate_opq_adaptive(l,xt,xq_all,xb,imp_idx,gt_all,all_skewness=all_skewness)\n",
        "\n",
        "hnsw = generate_hnsw([15],xt,xq_all,xb,imp_idx,gt_all,all_skewness=all_skewness)\n",
        "\n",
        "\n",
        "c_dir = \"./wiki_soccer/model\"\n",
        "if(not os.path.exists(c_dir)):\n",
        "  os.makedirs(c_dir)\n",
        "args = parser.parse_args(\"--lr_schedule 0.1,0.1,0.05,0.01 --num_learn 100000 --database bigann --update_freq 20 --val_freq 10 --lambda_triplet 10 --margin_k 10 --lambda_uniform 0.1 --lambda_lid 0.05 --lambda_seperation 0.01   --dint 1024 --dout 64 --size_base 5000000 --batch_size 512 --epochs 60 --save_best_criterion opqAdaptive_64,rank=100\".split())\n",
        "log_res = generate_result(l,xt,xq_all,xb,imp_idx,xb_imp,gt_all,args=args,dir=c_dir,all_skewness=all_skewness)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_data = np.load('./data/dpr_data/train_data.npy')\n",
        "train_labels = np.load('./data/dpr_data/train_labels.npy')\n",
        "test_data = np.load('./data/dpr_data/test_data.npy')\n",
        "test_labels = np.load('./data/dpr_data/test_labels.npy')\n",
        "\n",
        "all_gt = np.load('./data/dpr_data/gt.npy')\n",
        "\n",
        "print(\"finish loading\")\n",
        "np.random.seed(1234)\n",
        "tsize = 100000\n",
        "tidx = np.random.choice(train_data.shape[0], tsize, replace=False)\n",
        "\n",
        "\n",
        "xt_labels = train_labels[tidx]\n",
        "xb_labels = train_labels\n",
        "xb = train_data\n",
        "xt = train_data[tidx]\n",
        "\n",
        "imp_class = 2\n",
        "\n",
        "imp_idx = xt_labels == imp_class \n",
        "test_imp_idx = test_labels == imp_class \n",
        "xb_imp = xb_labels == imp_class \n",
        "  \n",
        "imp_test = np.nonzero(test_imp_idx)[0]\n",
        "imp_xb = np.nonzero(xb_imp)[0]\n",
        "\n",
        "dpr = []\n",
        "xq_all = {}\n",
        "gt_all = {}\n",
        "all_skewness = [0.7]\n",
        "l = [64]\n",
        "for skewness in all_skewness:\n",
        "  print(\"generating data for skwness {}\".format(skewness))\n",
        "  other_size = int(imp_test.shape[0]/skewness) - imp_test.shape[0]\n",
        "  other_test = np.setdiff1d(np.arange(test_data.shape[0]),imp_test)\n",
        "  other_idx = np.random.choice(other_test.shape[0], other_size , replace=False)\n",
        "  other_test = other_test[other_idx]\n",
        "  test_idx = np.concatenate([other_test,imp_test])\n",
        "\n",
        "  xq = test_data[test_idx]\n",
        "  print(xq.shape)\n",
        "  gt = all_gt[test_idx,:]\n",
        "  xq_all[skewness] = xq\n",
        "  gt_all[skewness] = gt\n",
        "\n",
        "opq_res = generate_opq(l,xt,xq_all,xb,imp_idx,gt_all,all_skewness=all_skewness)\n",
        "opq_adp = generate_opq_adaptive(l,xt,xq_all,xb,imp_idx,gt_all,all_skewness=all_skewness)\n",
        "\n",
        "# hnsw = generate_hnsw([15],xt,xq_all,xb,imp_idx,gt_all,all_skewness=all_skewness)\n",
        "\n",
        "\n",
        "c_dir = \"./dpr/model\"\n",
        "if(not os.path.exists(c_dir)):\n",
        "  os.makedirs(c_dir)\n",
        "args = parser.parse_args(\"--num_learn 100000 --database bigann --lambda_triplet 100 --margin_k 10 --lambda_uniform 0.01 --lambda_lid 0.1 --lambda_seperation 0.1 --update_freq 20 --dint 1024 --dout 64 --size_base 500000 --batch_size 512 --epochs 120 --save_best_criterion opqAdaptive_64,rank=100\".split())\n",
        "log_res = generate_result(l,xt,xq_all,xb,imp_idx,xb_imp,gt_all,args=args,dir=c_dir,all_skewness=all_skewness)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Spread.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.13 ('faissAdaptive')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "3929716e6faaad833afa28e758404c2cd8b41a631f07b838e2e944613cb28f68"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
